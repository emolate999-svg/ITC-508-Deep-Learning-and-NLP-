This exercise requires students to fine-tune an NLP model and analyze its performance through a structured workflow and formal reporting. First, students must convert the provided fine-tuning script into a fully organized Google Colab notebook, separating the code into logical sections—data preparation, model and tokenizer loading, training setup, finetuning, and inference—and providing detailed line-by-line explanations in corresponding markdown cells. After running the baseline model using default parameters, students will perform at least three hyperparameter experiments aimed at maximizing the model’s F1-Score, with each member responsible for modifying assigned hyperparameters and documenting results such as Accuracy and F1-Score in an Excel log sheet. Findings from all experiments must then be compiled into a short IEEE-formatted report that highlights the baseline performance, tests conducted, and the best-performing configuration. Final deliverables include the completed Colab notebook (.ipynb), the dataset used (.csv), the experiment log sheet (.xlsx), and the IEEE report (.docx), all to be submitted to the designated folder.
